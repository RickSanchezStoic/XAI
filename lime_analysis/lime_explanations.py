from typing import Dict, List

import numpy as np
import torch
from captum.attr import Lime


from lime_analysis.utils import measure_time, absolute_max_scaling


def initialize_lime(model: object) -> Lime:
    """
    Initialize the LIME (Local Interpretable Model-agnostic Explanations) instance
    for the given model using Captum.

    Args:
        model (object): A trained model that can be interpreted using LIME.
                        This should be a model that follows the standard PyTorch model
                        interface (e.g., a neural network subclass).

    Returns:
        Lime: A Captum Lime instance that provides local explanations for model predictions.
    """
    return Lime(model)


@measure_time
def generate_explanations(
    lime: Lime,
    test_data: torch.Tensor,
    true_labels: torch.Tensor,
    selected_indices: Dict[str, List[int]],
    n_samples: int = 50,
) -> Dict[str, Dict[str, torch.Tensor]]:
    """
    Generate and visualize explanations for selected indices in tabular data using LIME.

    Args:
        lime (Lime): An initialized Lime instance from the Captum library to generate explanations.
        test_data (torch.Tensor): A tensor containing the test data, with shape (num_samples, features).
        true_labels (torch.Tensor): A tensor containing the true labels corresponding to the test data.
        selected_indices (Dict[str, List[int]]): A dictionary where the keys are labels (e.g., 'TP', 'FP', 'FN', 'TN')
                                                 and the values are lists of indices of selected instances.
        n_samples (int, optional): The number of samples to generate for LIME. Default is 50.

    Returns:
        Dict[str, Dict[str, torch.Tensor]]: A dictionary where the keys are labels and the values are another dictionary
                                             containing the selected indices and corresponding attributions for each label.
    """
    attribution_dict = (
        {}
    )  # Initialize the dictionary to store attributions for each label

    for label, indices in selected_indices.items():
        if len(indices) > 0:  # Ensure there is at least one instance for the category
            idx = indices[0]  # Select the first instance for the given label
            input_tensor = (
                test_data[idx].unsqueeze(0).requires_grad_()
            )  # Add batch dimension
            target = true_labels[idx].item()

            # Generate LIME attributions
            attributions = lime.attribute(
                input_tensor, target=target, n_samples=n_samples
            )
            attributions_np = (
                attributions.squeeze().detach().numpy()
            )  # Convert to numpy for easier visualization

            # Store the attributions in the dictionary
            attribution_dict[label] = {
                "indices": indices,
                "attributions": absolute_max_scaling(
                    attributions_np
                ),  # Apply absolute max scaling
            }

    return attribution_dict


def parsimony(attributions: np.ndarray, threshold: float = 0.1) -> int:
    """
    Calculate the parsimony of the explanation, which is the number of features
    with importance scores above the given threshold.

    Args:
        attributions (np.ndarray): Array of scaled feature importance values generated by LIME.
                                    Each value corresponds to the importance of a feature.
        threshold (float, optional): The threshold above which a feature is considered important.
                                     Default is 0.1.

    Returns:
        int: The number of features with importance scores above the threshold.
    """
    # Count the number of features with importance scores above the threshold
    parsimony_count = np.sum(np.abs(attributions) > threshold)

    return parsimony_count
