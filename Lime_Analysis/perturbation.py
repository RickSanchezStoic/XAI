import torch
import numpy as np



def calculate_feature_means(dataset, feature_names):
    """
    Calculates the mean of each feature across the entire dataset.

    Args:
        dataset (Tensor or ndarray): The dataset (e.g., test dataset).
        feature_names (list): List of feature names.

    Returns:
        dict: A dictionary mapping feature names to their respective means.
    """
    if isinstance(dataset, np.ndarray):
        dataset = torch.tensor(dataset, dtype=torch.float32)

    feature_means = {feature: torch.mean(dataset[:, idx]).item() for idx, feature in enumerate(feature_names)}
    return feature_means


def perturb_single_instance(data_point, attributions, model, dataset, feature_names):
    """
    Perturbs a single instance and tracks when the prediction flips.

    Args:
        data_point (dict): Dictionary with label and index of the instance.
        attributions (Tensor or ndarray): Attributions generated by an explanation method.
        model (torch.nn.Module): The trained model.
        dataset (Tensor or ndarray): The dataset (e.g., test set).
        feature_names (list): List of feature names.

    Returns:
        dict: Dictionary containing flip steps and perturbed prediction values.
    """
    label, index = list(data_point.items())[0]
    data_point_values = dataset[index].clone()

    if isinstance(attributions, np.ndarray):
        attributions = torch.tensor(attributions, dtype=torch.float32)

    feature_means = calculate_feature_means(dataset, feature_names)
    sorted_indices = torch.argsort(attributions, descending=True)

    # Get the original prediction
    original_prediction = model(data_point_values.unsqueeze(0)).argmax().item()
    original_probs = torch.softmax(model(data_point_values.unsqueeze(0)), dim=1).detach().cpu().numpy().flatten()

    flip_steps = []
    perturbed_data = data_point_values.clone()

    for i, feature_idx in enumerate(sorted_indices, start=1):
        feature_name = feature_names[feature_idx]
        perturbed_data[0][feature_idx] = feature_means[feature_name]

        # Get the perturbed prediction
        perturbed_prediction = model(perturbed_data.unsqueeze(0)).argmax().item()

        # If the prediction flips, record the step
        if perturbed_prediction != original_prediction:
            flip_steps.append(i)


    return {'flip_steps': flip_steps, 'original_prediction': original_prediction,
            'perturbed_prediction': perturbed_prediction}





def perturb_dataset(dataset, attributions, model, feature_names):
    """
    Perturbs the entire dataset and computes accuracy after each perturbation step.

    Args:
        dataset (Tensor or ndarray): The dataset to perturb.
        attributions (Tensor or ndarray): Attributions generated by an explanation method.
        model (torch.nn.Module): The trained model.
        feature_names (list): List of feature names.

    Returns:
        dict: Perturbation steps and dataset accuracies.
    """
    if isinstance(attributions, np.ndarray):
        attributions = torch.tensor(attributions, dtype=torch.float32)

    sorted_indices = torch.argsort(attributions, descending=True)
    feature_means = calculate_feature_means(dataset, feature_names)

    accuracy_results = {'perturbation_steps': [], 'accuracies': []}
    original_predictions = model(dataset).argmax(dim=1)

    perturbed_dataset = dataset.clone()

    # Keep track of perturbed features to avoid redundant perturbations
    perturbed_features = set()

    for num_features in range(1, len(sorted_indices) + 1):
        # In each iteration, perturb the first 'num_features' features
        for i in range(num_features):
            feature_idx = sorted_indices[i]
            if feature_idx not in perturbed_features:
                feature_name = feature_names[feature_idx]
                perturbed_dataset[:, feature_idx] = feature_means[feature_name]
                perturbed_features.add(feature_idx)  # Mark the feature as perturbed

        perturbed_predictions = model(perturbed_dataset).argmax(dim=1)
        accuracy = (perturbed_predictions == original_predictions).float().mean().item()
        accuracy_results['perturbation_steps'].append(num_features)
        accuracy_results['accuracies'].append(accuracy)

    return accuracy_results



def compute_aupc(accuracy_results):
    """
    Computes the Area Under the Perturbation Curve (AUPC).

    Args:
        accuracy_results (dict): Perturbation results with accuracy over steps.

    Returns:
        float: The area under the perturbation curve.
    """
    steps = accuracy_results['perturbation_steps']
    accuracies = accuracy_results['accuracies']
    aupc = np.trapz(accuracies, steps)  # Numerically integrate
    return aupc


